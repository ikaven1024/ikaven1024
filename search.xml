<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[awesome]]></title>
    <url>%2F2018%2F04%2F09%2Fawesome%2F</url>
    <content type="text"><![CDATA[git配置1234567891011121314151617GIT_SSL_NO_VERIFY=truegit config --global http.sslVerify falsegit config --global core.autocrlf falsegit config --global core.safecrlf truegit config --global core.editor vimgit config --global color.diff autogit config --global color.status autogit config --global color.branch autogit config --global credential.helper store # 保存密码git config --global user.name &quot;xxx&quot;git config --global user.email &quot;xxx&quot;git config --global http.proxy xxxgit config --global http.postBuffer 52428800000git commit --amend --author=&quot;ikaven1024 &lt;ikaven1024@gmail.com&gt;&quot; 拉取部分代码12git config core.sparsecheckout true #开启sparse cloneecho &quot;devops&quot; &gt;&gt; .git/info/sparse-checkout #设置过滤条件，*表示所有，!表示匹配相反的 yum清缓存1yum makecache docker代理/usr/lib/systemd/system/docker.service: 1Environment=&quot;HTTP_PROXY=http://10.194.209.5:3128&quot; &quot;NO_PROXY=localhost,127.0.0.1,docker.hikvision.com.cn&quot; docker配置12sudo groupadd dockersudo usermod -aG docker $USER 12345678910111213mkdir -p /etc/docker &amp;&amp; cat &gt;/etc/docker/daemon.json &lt;&lt;-EOF&#123; &quot;host&quot;: &quot;unix:///var/run/docker.sock tcp://0.0.0.0:2375&quot;, &quot;insecure-registries&quot;: [&quot;xxx&quot;], &quot;registry-mirrors&quot;: [&quot;xxx&quot;], &quot;log-level&quot;: &quot;debug&quot;, &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;200m&quot;, &quot;max-file&quot;: &quot;1&quot; &#125;&#125;EOF Boot2Docker用户名密码docker tcuserroot docker用户下执行sudo -i k8s12kubectl cluster-infoprintf &quot;%-20s %-50s %-15s %-20s %-20s \n&quot; NameSpace &quot;Pod Name&quot; Status &quot;Pod IP&quot; &quot;Host IP&quot; `kubectl get pod --output go-template --template=&quot;&#123;&#123;range .items&#125;&#125; &#123;&#123;.metadata.namespace&#125;&#125; &#123;&#123;.metadata.name&#125;&#125; &#123;&#123;.status.phase&#125;&#125; &#123;&#123;.status.podIP&#125;&#125; &#123;&#123;.status.hostIP&#125;&#125;&#123;&#123;end&#125;&#125;&quot; --all-namespaces` 编译关键组件： 1make WHAT=&apos;cmd/kube-apiserver cmd/kube-controller-manager cmd/kubelet cmd/kube-proxy cmd/kubectl plugin/cmd/kube-scheduler&apos; RBDrbd –pool rbd –id admin -m 192.168.1.187 –key=AQA0xWtYn98iOxAAMSsJbhGwuxyFVg9aIAvFoA== ls 代码统计利器cloc SHELL1basepath=(cd `dirname 0`; pwd) go vendor123go get -u -v github.com/kardianos/govendorgovendor initgovendor add +external ansible123ansible localhost -m setupansible all -m shell -a date dns配置12/etc/resolv.confnameserver xxx.xxx.xxx.xxx screen12345// 进入screensu stackscript /dev/nullscreen -lsscreen -x stack]]></content>
      <tags>
        <tag>awesome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-source-read]]></title>
    <url>%2F2018%2F04%2F09%2Fk8s-source-read%2F</url>
    <content type="text"><![CDATA[系统架构 代码结构12345/-- |-- cmd/ |-- hack/ |-- pkg/ |-- pluging/ 源码阅读ServiceController。 Scheduler 参考]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-1.6-gpu]]></title>
    <url>%2F2018%2F04%2F09%2Fk8s-1-6-gpu%2F</url>
    <content type="text"><![CDATA[k8s 1.6中GPU调度验证1. 目的调研kubernetes 1.6版本中对GPU调度的支持，是否符合项目需求。 2. 环境物理环境：NVIDIA Tesla P4 *4nvdia驱动：NVIDIA-Linux-x86_64-375.51docker: 17.03.1-ceNVIDIA-docker: nvidia-docker-1.0.1-1.x86_64kubernetes: v1.6.4 3. 准备 安装NVIDIA驱动 这里需要注意依赖的版本不对，安装驱动时候会有问题。这里采用系统镜像IOS包中的rpm包。镜像挂载在/mnt目录。 新建本地源： 123456# vi /etc/yum.repos.d/local.repo[local_server]name=This is local repobaseurl=file:///mnt/enabeld=1gpgcheck=0 安装依赖 12yum install -y gcc kernel-develrpm -ivh /mnt/Packages/kernel-devel-3.10.0-514.el7.x86_64.rpm 安装驱动 1./NVIDIA-Linux-x86_64-375.51.run 准备库文件 在后面的GPU容器中需要挂载NVIDIA的驱动库。目前驱动文件分布比较零散。可以使用nvidia-docker帮我们收集库文件。 安装NVIDIA-Docker 1rpm -ivh nvidia-docker-1.0.1-1.x86_64.rpm 安装完后，需要启动nvidia-docker守护进程，并创建一个GPU容器。 123456789101112131415$ systemctl start nvidia-docker$ nvidia-docker run --rm nvidia/cuda nvidia-smiFri Jun 23 11:19:53 2017+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.51 Driver Version: 375.51 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P4 Off | 0000:02:00.0 Off | 0 || N/A 39C P8 7W / 75W | 0MiB / 7606MiB | 0% Default |+-------------------------------+----------------------+----------------------+| 1 Tesla P4 Off | 0000:03:00.0 Off | 0 || N/A 41C P8 7W / 75W | 0MiB / 7606MiB | 0% Default |+-------------------------------+----------------------+----------------------+ 可以看到生成了目录。这也是后面Pod需要挂载的目录。 12$ ls /var/lib/nvidia-docker/volumes/nvidia_driver/375.51bin lib lib64 这时候nvidia-docker就可以功成身退了。也可以卸载掉。 1systemctl stop nvidia-docker 这样我们就可以使用原始的docker启动GPU容器了： 123456789101112131415161718192021222324$ docker run --rm \ -v /var/lib/nvidia-docker/volumes/nvidia_driver/375.51:/usr/local/nvidia \ --device /dev/nvidiactl:/dev/nvidiactl \ --device /dev/nvidia-uvm:/dev/nvidia-uvm \ --device /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools \ --device /dev/nvidia0:/dev/nvidia0 \ nvidia/cuda nvidia-smiSat Aug 12 03:42:08 2017+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.51 Driver Version: 375.51 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P4 Off | 0000:04:00.0 Off | 0 || N/A 50C P0 24W / 75W | 0MiB / 7606MiB | 2% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ kubelet启动参数添加--feature-gates Accelerators=true，重启kubelet。 4. 验证部署yaml: 123456789101112131415161718192021# vi demo-1.yamlapiVersion: v1kind: Podmetadata: name: cuda-demo-1spec: containers: - name: cuda image: nvidia/cuda command: ["bash", "-c", "nvidia-smi &amp;&amp; while true; do sleep 1000; done"] imagePullPolicy: IfNotPresent resources: limits: alpha.kubernetes.io/nvidia-gpu: "1" volumeMounts: - name: nvidia-driver mountPath: /usr/local/nvidia volumes: - name: nvidia-driver hostPath: path: /var/lib/nvidia-docker/volumes/nvidia_driver/375.51 create后查看日志： 123456789101112131415161718# kubectl logs cuda-demo-1Mon Jun 26 03:11:06 2017+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.51 Driver Version: 375.51 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P4 Off | 0000:02:00.0 Off | 0 || N/A 37C P8 7W / 75W | 0MiB / 7606MiB | 0% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 表明给容器分配一个GPU卡成功。 在创建个cuda-demo-1资源量修改为2，查看Pod状态； 1234# kubectl get poNAME READY STATUS RESTARTS AGEcuda-demo-1 1/1 Running 0 6mcuda-demo-2 0/1 Pending 0 3s Pod处于Pending状态，通过describe，查看 123456# kubectl describe po cuda-demo-2...Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 27s 8 default-scheduler Warning FailedScheduling No nodes are available that match all of the following predicates:: Insufficient alpha.kubernetes.io/nvidia-gpu (3). 在我们的意料之内：因为GPU资源不足而调度失败。 删除cuda-demo-1后，cuda-demo-2正常running。 5. 补充 GPU配额值支持limits GPU在容器间隔离，并不共享 不支持GPU的部分调度，最小粒度是一张显卡 同化了GPU硬件，对于用户统一 6. 不足 需要预先准备好驱动库。目前通过nvidia-docker帮我们完成。 容易产生资源碎片化问题。 7. 结论相比kubernetes 1.3的GPU调度（一个主机只能支持一个GPU卡），1.6的调度有何很好的完善。基本概念都跟CPU、Memory一样，只不过GPU的最小粒度是卡。这在实际使用中会造成资源碎片问题，需要后续加强调度能力。基本满足目前的预言项目需求。 FAQ装了NV驱动后，没有生成/dev/nvidia-uvm，导致k8s不识别GPU.根据 https://www.zhihu.com/question/36588693 上的方法,运行一个Sample就行 http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>accelerator</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s响应]]></title>
    <url>%2F2018%2F04%2F09%2Fk8s-response%2F</url>
    <content type="text"><![CDATA[资源不存在12345678910111213141516171819# curl -i 127.0.0.1:8080/api/v1/namespaces/aaHTTP/1.1 404 Not FoundContent-Type: application/jsonDate: Fri, 16 Jun 2017 07:50:48 GMTContent-Length: 231&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123;&#125;, "status": "Failure", "message": "namespaces \"aa\" not found", "reason": "NotFound", "details": &#123; "name": "aa", "kind": "namespaces" &#125;, "code": 404&#125; 请求超时创建数据错误123456789101112131415# curl -i -X POST 127.0.0.1:8080/api/v1/namespacesHTTP/1.1 400 Bad RequestContent-Type: application/jsonDate: Fri, 16 Jun 2017 07:55:08 GMTContent-Length: 301&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123;&#125;, "status": "Failure", "message": "the object provided is unrecognized (must be of type Namespace): couldn't get version/kind; json parse error: unexpected end of JSON input (\u003cempty\u003e)", "reason": "BadRequest", "code": 400&#125; 创建已存在12345678910111213141516171819# curl -i -H "Content-type: application/json" -X POST -d '&#123;"apiVersion":"v1","kind":"Namespace","metadata":&#123;"name":"kube-system"&#125;&#125;' 127.0.0.1:8080/api/v1/namespacesHTTP/1.1 409 ConflictContent-Type: application/jsonDate: Fri, 16 Jun 2017 08:17:05 GMTContent-Length: 259&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123;&#125;, "status": "Failure", "message": "namespaces \"kube-system\" already exists", "reason": "AlreadyExists", "details": &#123; "name": "kube-system", "kind": "namespaces" &#125;, "code": 409&#125; 路径不合法12345678910111213141516# curl -i -X POST 127.0.0.1:8080/api/v1/namespaces/aaHTTP/1.1 405 Method Not AllowedContent-Type: application/jsonDate: Fri, 16 Jun 2017 07:53:01 GMTContent-Length: 229&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123;&#125;, "status": "Failure", "message": "the server does not allow this method on the requested resource", "reason": "MethodNotAllowed", "details": &#123;&#125;, "code": 405&#125; url path上的名称与数据不符合123456789101112131415# curl -i -H "Content-type: application/json" -X POST -d '&#123;"apiVersion":"v1","kind":"Pod","metadata":&#123;"name":"testpod","namespace":"noexists"&#125;,"spec":&#123;"containers":[&#123;"name":"nginx","image":"nginx:1.10.1-alpine"&#125;]&#125;&#125;' 127.0.0.1:8080/api/v1/namespaces/default/podsHTTP/1.1 400 Bad RequestContent-Type: application/jsonDate: Fri, 16 Jun 2017 08:20:16 GMTContent-Length: 228&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123;&#125;, "status": "Failure", "message": "the namespace of the provided object does not match the namespace sent on the request", "reason": "BadRequest", "code": 400&#125; 删除不存在资源12345678910111213141516171819# curl -i -X DELETE 127.0.0.1:8080/api/v1/namespaces/aaaHTTP/1.1 404 Not FoundContent-Type: application/jsonDate: Fri, 16 Jun 2017 08:37:49 GMTContent-Length: 233&#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123;&#125;, "status": "Failure", "message": "namespaces \"aaa\" not found", "reason": "NotFound", "details": &#123; "name": "aaa", "kind": "namespaces" &#125;, "code": 404&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[swarm-guide]]></title>
    <url>%2F2018%2F04%2F09%2Fswarm-guide%2F</url>
    <content type="text"><![CDATA[创建两个虚拟机12$ docker-machine create --driver virtualbox myvm1$ docker-machine create --driver virtualbox myvm2 登陆虚拟机，并以root权限修改配置 12$ docker-machine ssh myvm1$ sudo -i 创建Swarm集群12$ docker-machine ssh myvm1 &quot;docker swarm init --advertise-addr 192.168.99.100:2377&quot;$ docker-machine ssh myvm2 &quot;docker swarm join --token &lt;token&gt; &lt;ip&gt;:&lt;port&gt;&quot; 创建123456789101112131415161718192021$ vi getstartedlab.ymlversion: "3"services: web: image: docker pull nginx:1.10.1-alpine deploy: replicas: 5 resources: limits: cpus: "0.1" memory: 50M restart_policy: condition: on-failure ports: - "80:80" networks: - webnetnetworks: webnet:$ docker-machine scp getstartedlab.yml myvm1:~$ docker-machine ssh myvm1 "docker stack deploy -c getstartedlab.yml getstartedlab" 查看12345678$ docker-machine ssh myvm1 &quot;docker stack ps getstartedlab&quot;ID NAME IMAGE NODE DESIRED STATEjq2g3qp8nzwx test_web.1 username/repo:tag myvm1 Running88wgshobzoxl test_web.2 username/repo:tag myvm2 Runningvbb1qbkb0o2z test_web.3 username/repo:tag myvm2 Runningghii74p9budx test_web.4 username/repo:tag myvm1 Running0prmarhavs87 test_web.5 username/repo:tag myvm2 Running 服务访问 滚动升级1docker service update --image redis:3.0.7 redis 封锁节点12docker node update --availability drain &lt;NODE-ID&gt;docker node update --availability active &lt;NODE-ID&gt; 概念stack &gt; service &gt; task 12345docker stack ls # List all running applications on this Docker hostdocker stack deploy -c &lt;composefile&gt; &lt;appname&gt; # Run the specified Compose filedocker stack services &lt;appname&gt; # List the services associated with an appdocker stack ps &lt;appname&gt; # List the running containers associated with an appdocker stack rm &lt;appname&gt; # Tear down an application]]></content>
      <tags>
        <tag>docker</tag>
        <tag>swarm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 架构]]></title>
    <url>%2F2018%2F04%2F09%2Fdocker-arch%2F</url>
    <content type="text"><![CDATA[进程树：1234/usr/bin/dockerd └─docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock... └─docker-containerd-shim f195bb34b2a34e53f8d6... └─nginx: master process nginx -g daemon off; dockerdcontainerd是容器技术标准化之后的产物，为了能够兼容OCI标准，将容器运行时及其管理功能从Docker Daemon剥离。理论上，即使不运行dockerd，也能够直接通过containerd来管理容器。（当然，containerd本身也只是一个守护进程，容器的实际运行时由后面介绍的runC控制。） 1/usr/bin/dockerd docker-containerdcontainerd主要职责是镜像管理（镜像、元信息等）、容器执行（调用最终运行时组件执行）。 containerd向上为Docker Daemon提供了gRPC接口，使得Docker Daemon屏蔽下面的结构变化，确保原有接口向下兼容。向下通过containerd-shim结合runC，使得引擎可以独立升级，避免之前Docker Daemon升级会导致所有容器不可用的问题。 containerd独立负责容器运行时和生命周期（如创建、启动、停止、中止、信号处理、删除等） 1docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc docker-containerd-shim容器的代理，接受容器创建、删除等命令，管理容器进程。 1docker-containerd-shim f195bb34b2a34e53f8d6b38af405185e9ce3cf59c38a12545c1f1416bd7a162c /var/run/docker/libcontainerd/f195bb34b2a34e53f8d6b38af405185e9ce3cf59c38a12545c1f1416bd7a162c docker-runc runcOCI定义了容器运行时标准，runC是Docker按照开放容器格式标准（OCF, Open Container Format）制定的一种具体实现。 runC是从Docker的libcontainer中迁移而来的，实现了容器启停、资源隔离等功能。Docker默认提供了docker-runc实现，事实上，通过containerd的封装，可以在Docker Daemon启动的时候指定runc的实现。 containerd-shim通过run操作容器。创建完成后runc退出，容器进程由containerd-shim接管。容器内产生的孤儿进程也统一由containerd-shim接管。 使用runc运行容器12345678$ mkdir -p mycontainer/rootfs &amp;&amp; cd mycontainer# 生成rootfs$ docker export $(docker create nginx:1.10.1-alpine) | tar -C rootfs/ -xf -# 生成配置文件config.json$ docker-runc spec# 运行容器$ docker-runc run test/ # /* 进入容器 */]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 使用]]></title>
    <url>%2F2018%2F04%2F09%2Fdocker-use%2F</url>
    <content type="text"><![CDATA[版本说明 系统：CentOS Linux release 7.2.1511 (Core) 内核：3.10.0-327.el7.x86_64 docker信息： 1234Server Version: 17.06.2-ceStorage Driver: overlay Backing Filesystem: xfs Supports d_type: true 安装准备在基于xfs文件系统使用overlay存储驱动时，需要格式化参数ftype=1。否则会出现容器内目录删除失败的问题，见issue#31283。但是CentOS 7.2安装中并未使用这个参数。这里使用一个空分区来重新格式化。 如下，/dev/sdb1是新的分区，用mkfs进行格式化： 1mkfs.xfs -n ftype=1 /dev/sdb1 然后将此分区挂载到docker目录上（最好写入fstab中）： 12mkdir -p /var/lib/docker/mount /dev/sdb1 /var/lib/docker/ 可以使用以下命令验证 1xfs_info /dev/sdb1 | grep ftype 使用extfs也不行，见issues#15314，和https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.2_release_notes/technology-preview-file_systems 安装docker12wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-17.06.2.ce-1.el7.centos.x86_64.rpmyum install -y docker-ce-17.06.2.ce-1.el7.centos.x86_64.rpm 也可以使用阿里源http://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-17.06.2.ce-1.el7.centos.x86_64.rpm 写入配置1234567891011mkdir -p /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; "insecure-registries": ["xxxx"], "log-driver": "json-file", "log-opts": &#123; "max-size": "200m", "max-file": "1" &#125;&#125;EOF 启动docker12systemctl enable dockersystemctl start docker FAQDockerfile中yum install失败问题 在Overlay存储驱动下，镜像里如果吗，没有安装yum-plugin-ovl，会出现： 123456FROM centos:7.2.1511RUN curl http://mirrors.aliyun.com/help/centos/7/CentOS-Base.repo &gt; /etc/yum.repos.d/CentOS-Base.repoRUN yum makecache &amp;&amp; yum install -y perlRUN yum makecache &amp;&amp; yum install -y vim docker build后出现错误： 1Rpmdb checksum is invalid: dCDPT(pkg checksums): vim-filesystem.x86_64 2:7.4.160-2.el7 - u 分析 见issure#10180 解决1： 添加RUN yum install -y yum-plugin-ovl 解决2 RUN touch /var/lib/rpm/* &amp;&amp; RUN yum install -y &lt;packages-to-installed&gt; too many links问题 在Docker build或者pull时候，出现以下错误： 1/data/docker/runtime/overlay/49d96405b7d1ab2cc1c9f815347cf9716c3088b2db10ca3d36a894948b1e1a5c/root/var/lib/yum/yumdb/p/cf1f195f6bf8d654d694b941ead72c4c51798536-pcre-8.32-15.el7_2.1-x86_64/checksum_type /data/docker/runtime/overlay/1044d44c019cdd45df59c6df727dfef476decdc976477ab1124c0a0bf5735e10/tmproot559734426/var/lib/yum/yumdb/p/cf1f195f6bf8d654d694b941ead72c4c51798536-pcre-8.32-15.el7_2.1-x86_64/checksum_type: too many links 分析 由于硬链接数超出限制，见issues#32732 解决 清理镜像，减少镜像层数。 RLimit系统默认的是1024:4096。如果没有修改过，docker会调整到Hard Limit。如果想要更大的，可以： 修改/etc/docker/daemon.json 123456789&#123; "default-ulimits": &#123; "nofile": &#123; "Name": "nofile", "Hard": 2048, "Soft": 1024 &#125; &#125;&#125; 目前的版本中对这一配置有以下bug，见issues#1513： 1unable to configure the Docker daemon with file /etc/docker/daemon.json: the following directives don&apos;t match any configuration option: default-ulimits 已有PR#32547。 修改/usr/lib/systemd/system/docker.servicevice中添加 1234[Service]LimitNOFILE=65535LimitNPROC=65535LimitCORE=infinity 重启docker 添加启动参数 123dockerd --default-ulimit nofile=65535:65535 --default-ulimit nproc=65535:65535或dockerd --default-ulimit nofile=65535 --default-ulimit nproc=65535 单个容器设置 上面都是设置全局的。也允许单个容器的是设置 1docker run --ulimit nofile=65535 ...]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码编译docker]]></title>
    <url>%2F2018%2F04%2F09%2Fbuild-docker%2F</url>
    <content type="text"><![CDATA[编译环境 go 1.8.3 make gcc Protoc 3.x 12wget -c https://github.com/google/protobuf/releases/download/v3.1.0/protoc-3.1.0-linux-x86_64.zipunzip protoc-3.1.0-linux-x86_64.zip -d /usr/local 版本 COMPONENT COMMIT ID VERSION dockerd 89658bed64c2a8fe05a978e5b87dbec409d57a0f v17.05.0-ce containerd 9048e5e50717ea4497b757314bad98ea3763c145 v0.2.8 runc 9c2d8d184e5da67c95d601382adf14862e4f2228 libnetwork 7b2b1feb1de4817d522cc372af149ff48d25028e 编译全家桶123git clone https://github.com/moby/moby.git $GOPATH/src/github.com/docker/dockercd docker $GOPATH/src/github.com/docker/dockergit checkout -q 89658bed64c2a8fe05a978e5b87dbec409d57a0f moby项目提供了全家桶编译，需要在docker环境下以容器方式编译。这里需要注意两点： 代理。需要连到外网。如有需要请设置代理。 docker代理 Dockerfile中代理。修改项目中根目录下的Dockerfile文件，添加代理的环境变量。 1ENV http_proxy="http://yourproxy" https_proxy="http://yourproxy" golang下载地址。Dockerfile中使用的是https://golang.org/dl/go${GO_VERSION}.linux-amd64.tar.gz，属于墙外地址，如不能翻墙，可以替换为golangtc的地址 1RUN curl -fsSL "https://www.golangtc.com/static/go/$&#123;GO_VERSION&#125;/go$&#123;GO_VERSION&#125;.linux-amd64.tar.gz" 编译全家桶： 1make binary DOCKER_BUILD_APT_MIRROR=mirrors.aliyun.com 这里使用了aliyun镜像来加速编译。 生成在bundles/binary-daemon/ 1234567docker-containerddocker-containerd-ctrdocker-containerd-shimdockerddocker-initdocker-proxydocker-runc RPM同样需要在以下文件中设置代理 1234Dockerfileman/Dockerfilecontrib/builder/rpm/amd64/centos-7/Dockerfilehack/make/build-rpm 以及在hack/make/build-rpm中的cat &gt; &quot;$DEST/$version/Dockerfile.build&quot; &lt;&lt;-EOF位置上添加代理设置。 然后执行： 1make rpm DOCKER_BUILD_PKGS=centos-7 编译结果在： 1bundles/17.05.0-ce/build-rpm/centos-7/RPMS/x86_64/ 安装 cp $GOPATH/src/github.com/docker/docker/contrib/udev/80-docker.rules /etc/udev/rules.d/80-docker.rules 复制二进制到/usr/local/bin下 groupadd docker 运行dockerd启动。 分组件编译下面介绍各个组件的分别构建方式。 docker组件间的版本要求很苛刻。在项目中一般都有写明需要其他组件的commit id。 比如，在moby中的hack/dockerfile/binaries-commits和containerd中的RUNC.md。 cli12345git clone https://github.com/docker/cli.git $GOPATH/src/github.com/docker/clicd $GOPATH/src/github.com/docker/climake binarymv build/docker-linux-amd64 /usr/local/bin/docker containerd123456git clone https://github.com/containerd/containerd.git $GOPATH/src/github.com/containerd/containerdcd $GOPATH/src/github.com/containerd/containerdgit checkout -q 9048e5e50717ea4497b757314bad98ea3763c145make BUILDTAGS=no_btrfsmake install dockerd编译需要btrfs头文件和库。这里禁用了btr。 生成二进制： containerd containerd-shim containerd-stress ctr runc runc的版本必须跟containerd中要求的一致。见https://github.com/containerd/containerd/blob/master/RUNC.md，其中要求： RUNC_COMMIT = 9c2d8d184e5da67c95d601382adf14862e4f2228 12345678# 安装libseccompyum install -y libseccomp-develgit clone https://github.com/opencontainers/runc $GOPATH/src/github.com/opencontainers/runccd $GOPATH/src/github.com/opencontainers/runcgit checkout -q 9c2d8d184e5da67c95d601382adf14862e4f2228make &amp;&amp; make install 如果不希望启动seccomp，则 12&gt; make BUILDTAGS=''&gt;]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud 入门示例]]></title>
    <url>%2F2018%2F04%2F08%2FSpring-Cloud-Example%2F</url>
    <content type="text"><![CDATA[1. 服务注册中心 Eureka2. 服务注册3. 熔断器Hystrix4. 熔断器监控5. 配置中心6. 服务网关 Zuul7. 链路跟踪 Zipkin示例项目见：https://github.com/ikaven1024/SpringCloud-example]]></content>
      <categories>
        <category>springcloud</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springcloud</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow 安装]]></title>
    <url>%2F2018%2F04%2F03%2FTensorflow_install%2F</url>
    <content type="text"><![CDATA[Tensorflow 安装12345678910yum install -y python-pipmkdir /root/.pipcat &gt; /root/.pip/pip.conf &lt;&lt;EOF[global]index-url = http://10.1.14.235/python-pypi/simpletrusted-host = 10.1.14.235EOFpip install tesorflow TensorBoard12345678import tensorflow as tfa = tf.constant(2)b = tf.constant(3)x = tf.add(a, b)with tf.Session() as sess: writer = tf.summary.FileWriter('./graphs', sess.graph) print(sess.run(x))writer.close() # close the writer when you’re done using it tensorflow中一般有两步，第一步是定义图，第二步是在session中进行图中的计算。 常数12345678910111213141516171819202122232425262728# 常量tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)# 全零矩阵常量tf.zeros(shape, dtype=tf.float32, name=None)# 同纬全零矩阵常量tf.zeros_like(input_tensor, dtype=None, name=None, optimize=True)# 全1矩阵常量tf.ones(shape, dtype=tf.float32, name=None)# 同纬全1矩阵常量tf.ones_like(input_tensor, dtype=None, name=None, optimize=True)# tf.fill(dims, value, name=None)tf.linspace(start, stop, num, name=None)tf.range(start, limit=None, delta=1, dtype=None, name='range')# 随机数tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)# 截断正态分布tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None,name=None)tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None,name=None)tf.random_shuffle(value, seed=None, name=None)tf.random_crop(value, size, seed=None, name=None)tf.multinomial(logits, num_samples, seed=None, name=None)tf.random_gamma(shape, alpha, beta=None, dtype=tf.float32, seed=None, name=None) 变量1234x = tf.Variable()x.initializer # 初始化x.eval() # 读取里面的值x.assign() # 分配值给这个变量 在使用变量之前必须对其进行初始化，初始化可以看作是一种变量的分配值操作。 1234# 一次性初始化所有的变量sess.run(tf.global_variables_initializer())# 初始化部分变量sess.run(w.initializer) # w是定义好的变量 取出变量 1w.eval(session=sess) 占位符tensorflow中一般有两步，第一步是定义图，第二步是在session中进行图中的计算。对于图中我们暂时不知道值的量，我们可以定义为占位符，之后再用feed_dict去赋值。 1tf.placeholder(dtype, shape=None, name=None) 例子： 12345a = tf.placeholder(tf.float32, shape=[3])b = tf.constant([5, 5, 5], tf.float32)c = a + bwith tf.Session() as sess: print(sess.run(c, feed_dict=&#123;a: [1, 2, 3]&#125;)) 选择逻辑1234res = tf.where( tf.less(residual, delta), # 条件 0.5 * residual**2, # 条件满足时 delta * residual - 0.5 * delta**2) # 条件不满足时 线性回归123456789101112131415161718192021# 输入输出占位符X = tf.placeholder(tf.float32, shape=[], name='input') # shape=[]表示标量(scalar)Y = tf.placeholder(tf.float32, shape=[], name='label')# 参数w = tf.get_variable('weight', shape=[], initializer=tf.truncated_normal_initializer())b = tf.get_variable('bias', shape=[], initializer=tf.zeros_initializer())Y_predicted = w * X + bloss = tf.square(Y - Y_predicted, name='loss') # 均方误差# 会自动求导，然后更新参数optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for i in range(100): total_loss = 0 for x, y in data: # 样本数据放在data _, l = sess.run([optimizer, loss], feed_dict=&#123;X: x, Y: y&#125;) total_loss += l print("Epoch &#123;0&#125;: &#123;1&#125;".format(i, total_loss / n_samples)) 完整源码见这里 模型构建步骤构建计算图 定义输入和输出的占位符(placeholder) 定义模型中需要用到的权重 定义推断模型，构建网络 定义损失函数作为优化对象 定义优化器进行优化 执行构件图 第一次进行运算的时候，初始化模型的所有参数 传入训练数据，可以打乱顺序 网络前向传播，计算出当前参数下的网络输出 根据网络输出和目标计算出loss 通过loss方向传播更新网络中的参数 结构化1234with tf.name_scope(name_of_taht_scope): # declare op_1 # declare op_2 # ...]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>
